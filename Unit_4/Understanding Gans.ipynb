{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c814d99-b453-435e-a0ab-488ce87c87a3",
   "metadata": {},
   "source": [
    "# Understanding Generative Adversarial Networks (GANs)\n",
    "\n",
    "### A Quick Refresher on Generative models\n",
    "\n",
    "Before we can understasnd GANs, it is important to refresh our memories on what generative models are and how they are different from the classic discriminative methods we have been working with so far. \n",
    "\n",
    "If you recall the typicall paradigm of discriminative model has been to create a model that learns some conditional probability distribution $P(y|x)$ where $x$ is some input data and $y$ are your set of output labels. Essentially we are tring to find the probability of some label $y$ given some never before seen data $x$. Generative models however are different, these models seek to learn the joint probability distribution $P(x,y)$ of some data. In layman terms, we are essentially assuming that our data and label comes from some probability distribution and with this assumption in mind we can use this probability to generate samples from this same distribution.\n",
    "\n",
    "This is one of the most powerful feature of GANs and generative models as whole. The ability to create high quality data samples is a very powerful, and GANs have many applications based on this, from the creation of art, generation of realistic human faces, and more. \n",
    "\n",
    "| <img src=\"images/fake_ai_faces.0.png\" width=\"500\"/> |\n",
    "|:--:|\n",
    "| <b>Fig 1. Faces generated by GANs [6]</b>|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b834d6-fb92-4e2c-b2e7-2d068c03326c",
   "metadata": {},
   "source": [
    "### Let's talk random...\n",
    "\n",
    "Before we can begin to understand the formulation of GANs, it is important to understand the motivation and inspiration for the kind of problem we are trying to solve. Random variable generation in computers is one of these problems. By understanding random variable generation in computers I hope you'll be able to see how similar this is to the problem that the formulation of GANs seek to solve!\n",
    "\n",
    "With that said, computers are designed to be deterministic and consistent which makes it nigh if not impossible for them to truly generate random numbers. However as I am sure you know, most computers these days are able to randomly generate a number given some seed value. This technique is called psuedo-random number generation and with this, it is possible to have a computer that can generate a completely random sequence of numbers that follows uniform distribution. Of course there are many different generators and the implementations of psuedorandom number generation can be rather complex but for now all you need to know with certainty is that computers are easily capable of generating psuedo-random numbers that follow a uniform distribution. \n",
    "\n",
    "So being able to generate the uniform distribution is good for certain use cases, but often times having the flexibility to sample and generate new data points that follow a *different* distribution is also very important. This is called **psuedo-random number sampling**. Again like with psuedorandom number generation there are a variety of techniques to implement this from Monte Carlo Simulation to Acceptance Rejection, but the algorithm that we will focus on today is the *inverse transform method*. Using the inverse transform method, we can map our uniform distribution to more complex distributions and this will effectively allow us to generate data from interesting distributions using nothing but a uniform distribution.\n",
    "\n",
    "> The **inverse transform** is essentially a method in which to represent a complex random variable as a result of a function applied to a uniform random variable\n",
    "\n",
    "Before we can get into the inverse transform method, lets define the scope and some concepts needed for the formulation of this.\n",
    "\n",
    "Suppose I have $X$ as a complex random variable that I want to sample from, and $U$ as a uniform random variable over [0,1] that I know how to sample from. Note that a random variable can be represented by its Cumulative Distribution function (CDF). \n",
    "\n",
    "> A Cumulative Distribution function (CDF) is a function that represent the probability that a corresponding continuous random variable $X$, has a value less than or equal to some arguement of the function $x$. \n",
    "\n",
    "$$CDF_{x}(x)=\\mathbb{P}(X \\leq x) \\in[0,1]$$\n",
    "\n",
    "> For our uniform random variable $U$ this would be \n",
    "\n",
    "$$CDF_{U}(u)=\\mathbb{P}(U \\leq u)=u  \\in[0,1]$$\n",
    "\n",
    "With the general and uniform equations of CDF established we can begin to formulate the inverse transform method. So our goal is to find some transformation so we can turn our uniform distribution into a more complex distribution. For this examples sake, lets try to transform our uniform distribution to an exponential $\\lambda$ distribution using inverse transform sampling.\n",
    "\n",
    "> The CDF of the exponential distribution is given as follows:\n",
    "\n",
    "$$ F_{X}(x) = \\begin{cases} {1-e^{-\\lambda x}}, x \\geq 0 \\\\ 0, x < 0  \\end{cases} $$\n",
    "\n",
    "> Lets define our Transformation as $T$. This is what we want to find. We can write our objective as follows where U is our inform and X as the complex distribution we want to sample from:\n",
    "\n",
    "$$ T(U) = X $$\n",
    "\n",
    "> Combine this with the CDF to get the following:\n",
    "\n",
    "$$ CDF_{x}(x)=\\mathbb{P}(X \\leq x)=\\mathbb{P}(T(U) \\leq x) $$\n",
    "\n",
    "> Now apply the inverse of $T$ inside the parenthesis. Note the inverse of itself yields the argument.\n",
    "\n",
    "$$ =\\mathbb{P}(U \\leq T^{-1}(x)) $$\n",
    "\n",
    "> Now using the nature of the uniform distribution, we can simplify our equation to the following:\n",
    "\n",
    "$$ =\\mathbb{P}(U \\leq T^{-1}(x)) = T^{-1}(x) $$\n",
    "\n",
    "> Tying this all together this yields us the closed form solution for the Transformation we have been trying to solve for. \n",
    "\n",
    "$$ CDF_{x}(x) = T^{-1}(x) \\text{ or } CDF_{x}^{-1}(x) = T(x)$$\n",
    "\n",
    "With this, we just proved that the transformation $T$ that we are looking for can be found by getting the CDF of the distribution we want and calculating the inverse CDF of this function which will allow us to transform a uniform distribution into this new distribution. So with our example of the exponential distribution the inverse process would be as follows:\n",
    "\n",
    "$$ y = 1-e^{-\\lambda x} $$\n",
    "$$ 1 - y = e^{-\\lambda x} $$\n",
    "$$ ln(1-y)=-\\lambda x $$\n",
    "$$ x = \\frac{-ln(1-y)}{\\lambda} $$\n",
    "\n",
    "> Plugging in our $U$ for y this will yield our Inverse Transform equation\n",
    "\n",
    "$$ x = \\frac{-ln(1-U)}{\\lambda} $$\n",
    "\n",
    "$$ x = \\frac{-ln(U)}{\\lambda} \\text{Because U is symmetric}$$ \n",
    "\n",
    "With this we can transform our uniform distribution into an exponential lambda by passing in say 1000 random values from it into our Inverse Transform equation. So essentially as long as we know the CDF of a particular distribution, we can try to calculate the inverse of it and this will allow us to sample from distributions using nothing but a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6573b729-346f-422c-9bd9-2f93ae8661cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01921969843104665\n",
      "0.018363343123217937\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import expon\n",
    "\n",
    "point_count = 500\n",
    "\n",
    "x = np.random.uniform(0,1,point_count) # Generating 2000 points between 0 and 1.\n",
    "\n",
    "x_exponential = np.linspace(expon.ppf(0.01), expon.ppf(0.99), point_count)\n",
    "\n",
    "\n",
    "y_exponential_ivt = [ -1*np.log(val) for val in x] # Assuming rate lambda is 1 and perform inverse transform sampling\n",
    "y = [1 for x in range(0, point_count)]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax1.scatter(x,y) \n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('X~U[0,1]')\n",
    "ax1.set_title(\"Uniform Distribution Scatter Plot\")\n",
    "\n",
    "ax2.scatter(y_exponential_ivt,x) \n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title(\"Exponential Distribution Scatter Plot from IVT\")\n",
    "\n",
    "y_exponential = expon.pdf(x_exponential)\n",
    "\n",
    "ax3.plot(x_exponential, y_exponential,'r-', lw=5, alpha=0.6, label='expon pdf')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title(\"Exponential Distribution Scatter Plot from SciPy\")\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "nearest_ivt = find_nearest(y_exponential_ivt, 4)\n",
    "nearest_real = find_nearest(x_exponential, 4)\n",
    "print(x[nearest_ivt])\n",
    "print(y_exponential[nearest_real])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d53aa-987f-42e5-a243-9844773f99bf",
   "metadata": {},
   "source": [
    "### Tying this in with the formulation of GANs\n",
    "\n",
    "#### The problem of capturing nature and reality...\n",
    "\n",
    "Lets transition back to GANs. Lets say that I'm interested in generating black and white images of a face with a size of n by n. We can reshape this and represent this data as $N$, a vector consisting of each column of the image stacked on top of each other. We can say that due to the similarity of human faces, there exists some specific probability among the entire $N$ space that gives something that look like a face. We can apply this concept of similarity to other things such as birds, cars, etc as well.\n",
    "\n",
    "> **Essentially, the problem of finding this particular \"human face distribution\" that represents the faces of others over the $N$ dimensional vector space is the same as the problem explored and solved by Inverse Transform Sampling. Essentially we are trying to map some distribution that we do know, whether that be random, uniform, etc, into a distribution that represents what we are trying to generate, in this case faces.**\n",
    "\n",
    "#### Lets find a transform using neural networks!\n",
    "\n",
    "However, it is important to know, as with most things in nature, the probability distribution of things in nature is almost always very complex and is over a massive space. Even if we assume this distribution exists (aka there exists images that look like different dogs) we still would not be able to express this explicitly and this would make it very hard to generate images from this distribution! In terms of inverse transform, we cannot express this complex distribution as a CDF and therefore we would not be able to find an inverse transform of this distribution as well. *If only we could find a transform that allows us to transform a distribution we do know into one that could represent the more complex face distribution, we could simply use this function to generate new images of faces...*\n",
    "\n",
    "This is a problem for neural networks! By using neural networks we are able to model very complex functions and this gives us a chance to try to learn the transform function from the data that will allow us to generate images of human faces from a distribution that we do know. Thus the idea should be to design an architecture that would allow us to take in a simple random distribution and transform that into one that is more complex using some kind of generative neural network to learn this unknown transform from the data.\n",
    "\n",
    "### Archiectures and approaches to find this transform\n",
    "\n",
    "We need to design a network that can find this unknown transform. There are two approaches we can take to solve for this transform. **Direct and Indirect**. The Direct method follows the traditional neural network approach and consist of comparing the difference between the true versus the generated probability distribution and backpropagating the error through the network. This architecture is called the Generative Matching Networks (GMNs). The indirect approach very different. Instead of calculating the error and backpropagating it through the network, we train the network by using a downstream task that also result in the optimization and improvement of the generated distribution of the network. This is the approach used by GANs. For now, it is important to understand the direct approach before understanding why GANs work.\n",
    "\n",
    "### Generative Matching Networks\n",
    "\n",
    "As mentioned earlier, to even backpropagate, we need to first define our true labels. Off the bat the problems mentioned earlier when it comes to representing the true distribution, make it impossible for us to obtain a true label to compare to our generated distribution to. Because of this, direct comparisons are off the table and we must find a proxy that will allow us to do this. One way to tackle this problem is to use the probability distribution from a sample of images to act as the ground truth. Now that we have our ground truth, we now need a means to compare the distributions ot obtain an error. There are a variety of similarity measures such as KL-Divergence that can be used to compare two distributions, but Maximum Mean Discrepancy (MMD) is the most used approach.\n",
    "\n",
    "> Maximum Mean Discrepancy is a kernel based similarity metric that is used to determine the similarity between two distributions via the use of samples drawn from both distributions. There are multiple definitions for it that accomplish the same thing. The formulations of it are outside the scope of this notebook but if you would like to learn more you can find some helpful resources [here](http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBorRasSchSmo07.pdf), [here](http://www.gatsby.ucl.ac.uk/~gretton/papers/testing_workshop.pdf), and [here](https://www.kaggle.com/code/onurtunali/maximum-mean-discrepancy/notebook#Implementation-of-MMD).\n",
    "\n",
    "| <img src=\"images/GMN0.png\" width=\"500\"/> |\n",
    "|:--:|\n",
    "| <b>Fig 2. Overall training process of GMN! [1]</b>|\n",
    "\n",
    "Now that we have a way to compute error and we have defined our true labels, we can define a rough psuedo-code for how a network like this is going to work.\n",
    "\n",
    "##### The Training Process\n",
    "\n",
    "Given a random variable with a uniform distribution, we want the generated probability to be that of the human face distribution. To do so we can perform the following steps.\n",
    "\n",
    "1. Generate some uniform or normal variable to input into the model.\n",
    "2. Forward pass this noise into the model and obtain the generated output drawn from our predicted distribution\n",
    "3. Compare the true distribution versus our generated one based on the samples using MMD\n",
    "4. Backpropagate the error obtained to improve the networks performance.\n",
    "5. Repeat steps 1-4 until satisfied\n",
    "\n",
    "| <img src=\"images/GMN1.png\" width=\"500\"/> |\n",
    "|:--:|\n",
    "| <b>Fig 3. Overall generation process of GMN! [1]</b>|\n",
    "\n",
    "##### Limitations\n",
    "\n",
    "So while this approach is valid in tackling the problem of finding a particular transform for our human faces distribution, this method has limitations that hurt its ability to be performant. One of these is that MMD calculation can be very expensive. MMD in its calculation uses the kernel trick to compute the similiarity between distributions and this can be rather expensive given a large amount of data. Another is that selecting and fine tuning the right kernel can be tricky and even if you find the right one, the results of the direct method are more often than not worse than the indirect method...\n",
    "\n",
    "### Generative Adversarial Networks\n",
    "\n",
    "As mentioned previously, GANs are the indrect approach for finding the correct transform. Instead of solving directly trying to compute the error and backpropagating, we can indirectly accomplish this by increasing the error of a discriminator network which will help our seperate generator network perform better. The adversarial comes from the competition of these two competing networks.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "Before we can dive deep into the mathematics of GANs, lets define the upper level elements of a GAN. As stated before a GAN consist of two networks, a generator, whose purpose is to generate vector points that fools the discriminator increasing classification error, and a discriminator that tries to discern if the data it is receiving is fake or from the real dataset, decreasing classification error. \n",
    "\n",
    "The competition between the two networks is called Nash equilibrium and is a concept that stems from game theory, which is the mathematical study of competing rational agents.\n",
    "\n",
    "> **Nash equilibrium is defined as the common way to describe a solution in a game involving two competing players. Nash equilibrium is used to describe a condition in which each player knows the strateges of the other, and no player has any reason to change their own strategy.**\n",
    "\n",
    "#### How does this work? \n",
    "\n",
    "At first, it may seem unintuitive when one tries to understand why training a generator to fool a discriminator leads to the generator converging upon the target distribution. After all, this is a far more round about approach than the direct way. So for now lets forget the complex exmaple we set out to solve and solve this problem in one dimension. So suppose we have a true distribution ( A Gaussian) that we want to find. A direct approach taken by GMN would look like this.\n",
    "\n",
    "| <img src=\"images/GMN2.jpg\" width=\"500\"/> |\n",
    "|:--:|\n",
    "| <b>Fig 4. The one dimensional training process of a hypothetical GMN. The blue is the true distribution and the orange is the generated distribution. Obviously for such a simple example it is trivial to solve, but this demonstrates how the iterative direct process works in one dimension. [2]</b>|\n",
    "\n",
    "For adversarial our graph would look a little different as we now have a discriminator. The discriminator acts as a guide that tells the generator whether or not it is improving or not. So in this form, if the generator creates a distribution far away from the the true distribution, the discriminator would easily be able to tell and the generator must move its generated distribution closer towards the true distribution. When the two distributions are close to each other the Generator loss will be low and the discriminator will have a high loss as it struggles to distinguish the two distributions.\n",
    "\n",
    "TODO: Create graphic for this\n",
    "\n",
    "Intuitively this may seem like a very round-a-bout way of finding the transform of the true data. It definitely is a far more complicated process to follow compared to the direct approach but there are some advantages of finding the transform using this approach. First off, regardless of the complexity of this process, it still is much less complex than trying to compare the two probability distributions using samples per the direct method. \n",
    "\n",
    "#### Training process\n",
    "\n",
    "So in the case of GANs, the generator and discriminator are locked into a zero-sum competing game where one of the networks wins and the other loses. In a sense, the generator will be improving based on the error provided by the discriminator, and the discriminator will try to get better through a direct approach using the labeled fake or real images. However, it is important to note that when backpropagating the error provided by the discriminator that we freeze the weights of the discriminator, to prevent the weights from changing in the discriminator which would make it hard for the generator to converge. With this in mind, we can define the psuedo-code we would take when training a GAN:\n",
    "\n",
    "1. Generator and Discriminator architecture are created.\n",
    "2. Generate some uniform or normal variable that is passed into the generator, obtaining the generated output drawn from our predicted distribution\n",
    "3. Pass the generated output along with the real images to the discriminator where it will attempt to discern the difference between both sets of data.\n",
    "4. Obtain a discriminator loss based on the performance of the discriminator in classifying both sets of data and a generator loss based on how well the GAN fooled the discriminator.\n",
    "5. Backpropagate the discriminator loss through the discriminator.\n",
    "6. Freeze the weights of the discriminator model\n",
    "7. Backpropagate the generator loss, through the frozen discriminator and to the generator.\n",
    "8. Repeat steps 2 - 7 until convergence or until epoch reached\n",
    "\n",
    "#### Derivations\n",
    "\n",
    "Now that we understand, the high level overview and implementation of GANs, lets intuitively try to generate the objective function of our model. So because our approach is a game theory one that seeks to find the Nash equilibrium between two models, the approach taken by the author is Minimax optimization. Lets start with the discriminator\n",
    "\n",
    "> Minimax is a decision rule that essentially says our goal should be to minimize the possible loss for the worse case while maximizing gain. While Minimax optimization is important to learn, a higher level overview is often the minimum needed to understand how it is applied to GANS. If you want more information about it you can find it [here](https://towardsdatascience.com/game-theory-minimax-f84ee6e4ae6e).\n",
    "\n",
    "Using Minimax optimization this would yield us the following equation where we seek to minimize the generator's loss using weights $x$ while maximizing the loss of the discriminator $y$ as measured by our loss function $f(x,y)$. This will yield a strong generator which is our goal.\n",
    "\n",
    ">$$ min_{x} max_{y} f(x,y) $$\n",
    ">$$ f(x,y) :=  \\mathbb{E}[ f_{\\zeta, \\xi}(x,y)] $$\n",
    ">where $\\zeta$ (Zeta) is a random real image from the dataset, and $\\xi \\sim N(0,I_{d})$ (Xi) is a noise vector which the generator attempts to find a transformation for that maps this to the real distribution.\n",
    "\n",
    "Now we have the choice to select several loss functions that we will use to compute the error of our GAN. The one chosen by Goodfellow in the formulation of GANs is the cross-entropy loss function. Which is equivalent to the following:\n",
    "\n",
    "$$ f_{\\zeta, \\xi}(x,y) := \\log(\\mathcal{D}(y;\\zeta)) + \\log(1-\\mathcal{D}(y;\\mathcal{G}(x;\\xi))) $$\n",
    "\n",
    "\n",
    "Here we are maximizing the odds that the discriminator is able to properly discriminate real images given weight $y$, while also maximizing its ability to discriminate between the generated outputs given weight $x$ and noise $\\xi$\n",
    "\n",
    "The generator would work the opposite of this. \n",
    "\n",
    "$$ f_{\\xi}(x,y) :=  \\log(\\mathcal{D}(y;\\mathcal{G}(x;\\xi))) $$\n",
    "\n",
    "For the generator it just seesks to minimize the discriminators ability to detect its fake images (maximize the likelihood that it fools the discriminator).\n",
    "\n",
    "#### Code example of simple GAN [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4d0fe4-0030-460b-8ebb-0678ec43b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_y(x): # Squares the inputted x value giving the parabolic shape\n",
    "    return 10 + x*x\n",
    "\n",
    "\n",
    "def sample_data(n=10000, scale=100): # Function to generate random parabolic shaped data where n is amount of points and scale is the range of the x and y axis\n",
    "    data = []\n",
    "\n",
    "    x = scale*(np.random.random_sample((n,))-0.5) # returns array x with random numbers between 0.5 and -0.5\n",
    "\n",
    "    for i in range(n):\n",
    "        yi = get_y(x[i])\n",
    "        data.append([x[i], yi])\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "data = sample_data(2000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2612b2-993a-4bfb-8d1e-9c8ba445d02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2ad630698eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(1)\n",
    "plt.title(\"Simple data\", fontsize='16')\t#title\n",
    "plt.scatter(data[:,0], data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e74962-6f30-4637-8cca-8353dd090f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(Z,hsize=[16, 16],reuse=False): # Z is the random samples, hsize is the number of units in each hidden layer, and reuse is a variable that specifies whether to reuse the same layers or not\n",
    "    with tf.variable_scope(\"GAN/Generator\",reuse=reuse):\n",
    "        h1 = tf.layers.dense(Z,hsize[0],activation=tf.nn.leaky_relu)\n",
    "        h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu)\n",
    "        out = tf.layers.dense(h2,2) # Output is a 2 dimensional vector which is our generated probability that we are trying to learn\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0704b851-5f95-4fe8-96c5-184f18343c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X,hsize=[16, 16],reuse=False): # X is the real data, hsize is the number of units in each hidden layer, and reuse is a variable that specifies whether to reuse the same layers or not\n",
    "    with tf.variable_scope(\"GAN/Discriminator\",reuse=reuse):\n",
    "        h1 = tf.layers.dense(X,hsize[0],activation=tf.nn.leaky_relu) \n",
    "        h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu)\n",
    "        h3 = tf.layers.dense(h2,2) # Fixed to 2 as we are working in 2d\n",
    "        out = tf.layers.dense(h3,1) # Output is a logit prediction for the given X belonging to real or fake and the feature transformation learned by the discriminator\n",
    "\n",
    "    return out, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb08283-5704-426e-9744-8766d58ff942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /apps/tensorflow/2.6.0/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /apps/tensorflow/2.6.0/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/tensorflow/2.6.0/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "/apps/tensorflow/2.6.0/lib/python3.9/site-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,2]) #TF variables that define X and Z for the real and random noise samples\n",
    "Z = tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "G_sample = generator(Z) # Define our graph \n",
    "r_logits, r_rep = discriminator(X) \n",
    "f_logits, g_rep = discriminator(G_sample,reuse=True)\n",
    "\n",
    "# Obtain the loss of the generator and discriminator\n",
    "disc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,labels=tf.ones_like(r_logits)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.zeros_like(f_logits)))\n",
    "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.ones_like(f_logits)))\n",
    "\n",
    "# Initialize variables needed for optimizer\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\")\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Discriminator\")\n",
    "\n",
    "gen_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(gen_loss,var_list = gen_vars) # G Train step\n",
    "disc_step = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(disc_loss,var_list = disc_vars) # D Train step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b638c8-1e9b-40d9-81d1-13fe0ddea2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-30 14:43:10.956419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:/opt/slurm/lib64:\n",
      "2022-03-30 14:43:10.956464: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-30 14:43:10.956491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c0704a-s2.ufhpc): /proc/driver/nvidia/version does not exist\n",
      "2022-03-30 14:43:10.956791: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 0\t Discriminator loss: 0.8842\t Generator loss: 0.6982\n",
      "Iterations: 100\t Discriminator loss: 0.0000\t Generator loss: 16.2080\n",
      "Iterations: 200\t Discriminator loss: 0.0000\t Generator loss: 17.3530\n",
      "Iterations: 300\t Discriminator loss: 19.5265\t Generator loss: 0.0005\n",
      "Iterations: 400\t Discriminator loss: 9.8264\t Generator loss: 18.0756\n",
      "Iterations: 500\t Discriminator loss: 4.7890\t Generator loss: 2.1319\n",
      "Iterations: 600\t Discriminator loss: 9.6424\t Generator loss: 0.0000\n",
      "Iterations: 700\t Discriminator loss: 16.5640\t Generator loss: 0.0000\n",
      "Iterations: 800\t Discriminator loss: 9.5290\t Generator loss: 0.0000\n",
      "Iterations: 900\t Discriminator loss: 22.0920\t Generator loss: 0.1487\n",
      "Iterations: 1000\t Discriminator loss: 9.5871\t Generator loss: 0.0000\n",
      "Iterations: 1100\t Discriminator loss: 6.0033\t Generator loss: 0.0003\n",
      "Iterations: 1200\t Discriminator loss: 3.3651\t Generator loss: 0.0102\n",
      "Iterations: 1300\t Discriminator loss: 1.3537\t Generator loss: 0.6674\n",
      "Iterations: 1400\t Discriminator loss: 1.4233\t Generator loss: 0.5071\n",
      "Iterations: 1500\t Discriminator loss: 1.2179\t Generator loss: 0.4852\n",
      "Iterations: 1600\t Discriminator loss: 1.1521\t Generator loss: 0.6242\n",
      "Iterations: 1700\t Discriminator loss: 1.2449\t Generator loss: 0.6418\n",
      "Iterations: 1800\t Discriminator loss: 1.3464\t Generator loss: 0.6276\n",
      "Iterations: 1900\t Discriminator loss: 1.2667\t Generator loss: 0.6484\n",
      "Iterations: 2000\t Discriminator loss: 1.1894\t Generator loss: 0.8224\n",
      "Iterations: 2100\t Discriminator loss: 1.2188\t Generator loss: 0.6785\n",
      "Iterations: 2200\t Discriminator loss: 1.1558\t Generator loss: 0.7662\n",
      "Iterations: 2300\t Discriminator loss: 1.2476\t Generator loss: 0.7238\n",
      "Iterations: 2400\t Discriminator loss: 1.2460\t Generator loss: 0.6446\n",
      "Iterations: 2500\t Discriminator loss: 1.2292\t Generator loss: 0.6418\n",
      "Iterations: 2600\t Discriminator loss: 1.1952\t Generator loss: 0.6705\n",
      "Iterations: 2700\t Discriminator loss: 1.2505\t Generator loss: 0.5973\n",
      "Iterations: 2800\t Discriminator loss: 1.2467\t Generator loss: 0.6077\n",
      "Iterations: 2900\t Discriminator loss: 1.2245\t Generator loss: 0.6591\n",
      "Iterations: 3000\t Discriminator loss: 1.2151\t Generator loss: 0.6796\n",
      "Iterations: 3100\t Discriminator loss: 1.2105\t Generator loss: 0.6508\n",
      "Iterations: 3200\t Discriminator loss: 1.0796\t Generator loss: 0.6499\n",
      "Iterations: 3300\t Discriminator loss: 1.1905\t Generator loss: 0.8086\n",
      "Iterations: 3400\t Discriminator loss: 1.2415\t Generator loss: 0.6891\n",
      "Iterations: 3500\t Discriminator loss: 1.2282\t Generator loss: 0.7077\n",
      "Iterations: 3600\t Discriminator loss: 1.2127\t Generator loss: 0.6327\n",
      "Iterations: 3700\t Discriminator loss: 1.2012\t Generator loss: 0.6800\n",
      "Iterations: 3800\t Discriminator loss: 1.1918\t Generator loss: 0.8247\n",
      "Iterations: 3900\t Discriminator loss: 1.2340\t Generator loss: 0.6868\n",
      "Iterations: 4000\t Discriminator loss: 1.1239\t Generator loss: 0.7272\n",
      "Iterations: 4100\t Discriminator loss: 1.2404\t Generator loss: 0.6970\n",
      "Iterations: 4200\t Discriminator loss: 1.2523\t Generator loss: 0.8491\n",
      "Iterations: 4300\t Discriminator loss: 1.2599\t Generator loss: 0.6147\n",
      "Iterations: 4400\t Discriminator loss: 1.1407\t Generator loss: 0.7256\n",
      "Iterations: 4500\t Discriminator loss: 1.1444\t Generator loss: 0.6382\n",
      "Iterations: 4600\t Discriminator loss: 1.2265\t Generator loss: 0.7777\n",
      "Iterations: 4700\t Discriminator loss: 1.2837\t Generator loss: 0.7558\n",
      "Iterations: 4800\t Discriminator loss: 1.1897\t Generator loss: 0.7167\n",
      "Iterations: 4900\t Discriminator loss: 1.2028\t Generator loss: 0.7427\n",
      "Iterations: 5000\t Discriminator loss: 1.1901\t Generator loss: 0.8765\n",
      "Iterations: 5100\t Discriminator loss: 1.1782\t Generator loss: 0.8138\n",
      "Iterations: 5200\t Discriminator loss: 1.1735\t Generator loss: 0.9251\n",
      "Iterations: 5300\t Discriminator loss: 1.1949\t Generator loss: 0.8171\n",
      "Iterations: 5400\t Discriminator loss: 1.2293\t Generator loss: 0.6533\n",
      "Iterations: 5500\t Discriminator loss: 1.0837\t Generator loss: 0.6375\n",
      "Iterations: 5600\t Discriminator loss: 1.2178\t Generator loss: 0.7457\n",
      "Iterations: 5700\t Discriminator loss: 1.1529\t Generator loss: 0.7441\n",
      "Iterations: 5800\t Discriminator loss: 1.1793\t Generator loss: 0.8542\n",
      "Iterations: 5900\t Discriminator loss: 1.2205\t Generator loss: 0.6284\n",
      "Iterations: 6000\t Discriminator loss: 1.2132\t Generator loss: 0.7588\n",
      "Iterations: 6100\t Discriminator loss: 1.1792\t Generator loss: 0.7372\n",
      "Iterations: 6200\t Discriminator loss: 1.1517\t Generator loss: 0.6064\n",
      "Iterations: 6300\t Discriminator loss: 1.1569\t Generator loss: 0.6377\n",
      "Iterations: 6400\t Discriminator loss: 1.1128\t Generator loss: 0.4737\n",
      "Iterations: 6500\t Discriminator loss: 1.2580\t Generator loss: 0.8980\n",
      "Iterations: 6600\t Discriminator loss: 1.1992\t Generator loss: 0.8450\n",
      "Iterations: 6700\t Discriminator loss: 1.2597\t Generator loss: 0.7178\n",
      "Iterations: 6800\t Discriminator loss: 1.1157\t Generator loss: 0.8271\n",
      "Iterations: 6900\t Discriminator loss: 1.2407\t Generator loss: 0.8703\n",
      "Iterations: 7000\t Discriminator loss: 1.1364\t Generator loss: 0.7358\n",
      "Iterations: 7100\t Discriminator loss: 1.2023\t Generator loss: 0.9276\n",
      "Iterations: 7200\t Discriminator loss: 1.0820\t Generator loss: 0.6543\n",
      "Iterations: 7300\t Discriminator loss: 1.0796\t Generator loss: 0.7939\n",
      "Iterations: 7400\t Discriminator loss: 1.0933\t Generator loss: 0.5330\n",
      "Iterations: 7500\t Discriminator loss: 1.1775\t Generator loss: 0.8090\n",
      "Iterations: 7600\t Discriminator loss: 1.1973\t Generator loss: 1.2950\n",
      "Iterations: 7700\t Discriminator loss: 1.1326\t Generator loss: 0.6547\n",
      "Iterations: 7800\t Discriminator loss: 1.0064\t Generator loss: 0.5633\n",
      "Iterations: 7900\t Discriminator loss: 1.1943\t Generator loss: 0.7236\n",
      "Iterations: 8000\t Discriminator loss: 1.1545\t Generator loss: 0.8688\n",
      "Iterations: 8100\t Discriminator loss: 1.1239\t Generator loss: 0.6864\n",
      "Iterations: 8200\t Discriminator loss: 1.1674\t Generator loss: 0.7753\n",
      "Iterations: 8300\t Discriminator loss: 1.2335\t Generator loss: 0.8430\n",
      "Iterations: 8400\t Discriminator loss: 1.2196\t Generator loss: 1.0069\n",
      "Iterations: 8500\t Discriminator loss: 1.2312\t Generator loss: 0.5772\n",
      "Iterations: 8600\t Discriminator loss: 1.1295\t Generator loss: 0.5945\n",
      "Iterations: 8700\t Discriminator loss: 1.1023\t Generator loss: 0.6597\n",
      "Iterations: 8800\t Discriminator loss: 1.1477\t Generator loss: 0.7833\n",
      "Iterations: 8900\t Discriminator loss: 1.1676\t Generator loss: 0.7604\n",
      "Iterations: 9000\t Discriminator loss: 1.2414\t Generator loss: 0.7708\n",
      "Iterations: 9100\t Discriminator loss: 1.2419\t Generator loss: 0.6438\n",
      "Iterations: 9200\t Discriminator loss: 1.1402\t Generator loss: 0.5912\n",
      "Iterations: 9300\t Discriminator loss: 1.2959\t Generator loss: 0.8532\n",
      "Iterations: 9400\t Discriminator loss: 1.2913\t Generator loss: 0.6421\n",
      "Iterations: 9500\t Discriminator loss: 1.2313\t Generator loss: 0.7362\n",
      "Iterations: 9600\t Discriminator loss: 1.0217\t Generator loss: 0.5741\n",
      "Iterations: 9700\t Discriminator loss: 1.1834\t Generator loss: 0.7464\n",
      "Iterations: 9800\t Discriminator loss: 1.2065\t Generator loss: 1.0387\n",
      "Iterations: 9900\t Discriminator loss: 1.2884\t Generator loss: 0.7882\n",
      "Iterations: 10000\t Discriminator loss: 1.1682\t Generator loss: 1.0111\n"
     ]
    }
   ],
   "source": [
    "# sess = tf.Session(config=config)\n",
    "sess = tf.Session()\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "\n",
    "# Params\n",
    "batch_size = 256\n",
    "nd_steps = 10\n",
    "ng_steps = 10\n",
    "\n",
    "# Helper functions ot sample from uniform distribution\n",
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "# Store the original data for plotting\n",
    "x_plot = sample_data(n=batch_size)\n",
    "\n",
    "for i in range(10001):\n",
    "    X_batch = sample_data(n=batch_size)\n",
    "    Z_batch = sample_Z(batch_size, 2)\n",
    "\n",
    "    # First calculate discriminator loss\n",
    "    for _ in range(nd_steps):\n",
    "        _, dloss = sess.run([disc_step, disc_loss], feed_dict={X: X_batch, Z: Z_batch})\n",
    "    rrep_dstep, grep_dstep = sess.run([r_rep, g_rep], feed_dict={X: X_batch, Z: Z_batch})\n",
    "\n",
    "    # Then calculate generator loss\n",
    "    for _ in range(ng_steps):\n",
    "        _, gloss = sess.run([gen_step, gen_loss], feed_dict={Z: Z_batch})\n",
    "\n",
    "    rrep_gstep, grep_gstep = sess.run([r_rep, g_rep], feed_dict={X: X_batch, Z: Z_batch})\n",
    "\n",
    "    if(i%100==0):\n",
    "        print (\"Iterations: %d\\t Discriminator loss: %.4f\\t Generator loss: %.4f\"%(i,dloss,gloss))\n",
    "\n",
    "\n",
    "    if i%1000 == 0:\n",
    "        plt.figure()\n",
    "        g_plot = sess.run(G_sample, feed_dict={Z: Z_batch})\n",
    "        xax = plt.scatter(x_plot[:,0], x_plot[:,1])\n",
    "        gax = plt.scatter(g_plot[:,0],g_plot[:,1])\n",
    "\n",
    "        plt.legend((xax,gax), (\"Real Data\",\"Generated Data\"))\n",
    "        plt.title('Samples at Iteration %d'%i)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/iterations/iteration_%d.png'%i)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73a986c-60d9-4ba3-a021-4f0a04abf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # Simple utility to convert images to gifs\n",
    "import numpy as np\n",
    "import imageio\n",
    "ipath = \"plots/iterations/iteration_%d.png\"\n",
    "\n",
    "images = []\n",
    "for i in range(11):\n",
    "    images.append(imageio.imread(ipath%(i*1000)))\n",
    "imageio.mimsave('gan_results/iterations.gif', images, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669a682-ea4c-49dc-a48c-f7a0b4fcfc74",
   "metadata": {},
   "source": [
    "#### GAN problems\n",
    "\n",
    "While GANs are a wonderful way to find some complex distribution we can sample from, they are also known to be challenging and difficult to train as well. Some of the issues often found during GAN training are as follows:\n",
    "\n",
    "- So we know that the generator is driven by the loss of the discriminator as it is used to backpropagate the generators gradient. So initially in an ideal state, the discriminator will be able to easily discern between the fake and real images, however, as the generator improves the discriminator may start to struggle (as it should) in discriminating between the two classes. At this point, the discriminator should have a probability of prediction of 50% indicating that the discriminator can no londer discern the two classes and is beaten by the generator. Training must be stopped at this point or else the generator will begin to train using the discriminators unreliable and usually incorrect results.\n",
    "\n",
    ">Solution: Implement early stopping! Attatch a callback function to the training loop that looks for a certain threshold to be reached (Loss, FID, etc.) and stop training when this threshold is reached!\n",
    "\n",
    "- If a discriminator converges to quickly and trains too fast compared to the generator, this causes the GAN to enter into a suboptimal state. the discriminator loss will be very small at this point and this in turn, causes the GAN to try to backpropagate a very small loss to the generator which will fail to update the weights due to the vanishing gradient problem. \n",
    "\n",
    ">Solution: Utilize Wasserstein loss, or modify the minimax loss to deal with vanishing gradients\n",
    "\n",
    "- The last and most well known problem of GANs is **mode collapse**. Sometimes the discriminator may be stuck in a local minimum and fails to optimize further. This can also occur as a result of the vanishing gradient problem. But essentially because the discriminator is at a weakened suboptimal state, it may not be able to correctly classify certain instances correctly. The generator will learn from this the discriminator is weak to certain instances or classes and will train exclusively on generating the same class over and over to fool the discriminator. This will lead to the generator becoming over trained at reproducing one class and overoptimization.\n",
    "\n",
    ">Solution: Utilize Wasserstein loss, or used Unrolled GANs which are GANs that use a generator loss function that incorporates the discriminator's classifications, along with the output of future discriminator versions. This prevents the discriminator from being over-optimized and allows it to move around and escape mode collapse better.\n",
    "\n",
    "To solve these problems we can modify the objective function of our GAN to counteract the effects of mode collapse or the vanishing gradient problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226c7fa-f377-412e-bb79-92ae10d11b88",
   "metadata": {},
   "source": [
    "#### References\n",
    "1. https://towardsdatascience.com/introduction-to-generative-networks-e33c18a660dd\n",
    "2. https://medium.com/towards-data-science/understanding-generative-adversarial-networks-gans-cd6e4651a29\n",
    "3. https://christophm.github.io/interpretable-ml-book/proto.html\n",
    "4. https://blog.paperspace.com/implementing-gans-in-tensorflow/\n",
    "5. http://www.offconvex.org/2020/07/06/GAN-min-max/\n",
    "6. https://medium.com/voice-tech-podcast/fake-people-the-newest-ai-technology-sold-to-companies-e8ac214bf05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
