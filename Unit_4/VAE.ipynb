{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preface\n",
    "(Basically notes  from here:)<br>\n",
    "Guide by original authors: https://arxiv.org/pdf/1906.02691.pdf\n",
    "\n",
    "A major division in ML is generative vs discriminative models. Lets consider both from the context of modeling fruit pictures<br>\n",
    "*Convention:*<br>\n",
    "*x - the image of the fruit*\n",
    "*z - the label of the fruit (also the distribution of that fruit)*\n",
    "*   **discriminative** - model learns p(z|x)<br>\n",
    "*   **generative** - model learns p(x|z)<br>\n",
    "generative models can still be used for classification, but it requires using bayes rule which can be computationally expensive. Also generative models tend to make more assumptions about the underlying structures in data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Questions\n",
    "*   Why is bayes rule intractable in machine learning?\n",
    "*   Will AEs converge to VAEs if the dimmensionality of the latent space is small?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variational Autoencoder\n",
    "Original Paper: https://arxiv.org/pdf/1312.6114.pdf <br>\n",
    "Guide by original authors: https://arxiv.org/pdf/1906.02691.pdf\n",
    "Shoutouts:\n",
    "* https://www2.bcs.rochester.edu/sites/jacobslab/cheat_sheet/VariationalAutoEncoder.pdf\n",
    "* https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$z -> x -> p(z|x) -> \\hat{z} -> p(x|\\hat{z}) -> \\hat{x}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KL Divergence\n",
    "Motivation: As a distance metric to quantify the difference between two distributions. <br>\n",
    "Key features:\n",
    "* Independent of normalization factor\n",
    "* Independent of scale factors as well"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Derivation 1\n",
    "\n",
    "| | D1 | D2 |\n",
    "| --- | --- | --- |\n",
    "|p(a)| .8| .3|\n",
    "|p(b)| .1| .3|\n",
    "|p(c)| .1| .4|\n",
    "\n",
    "Some sequence X **taken from D1**: abcba\n",
    "\n",
    "Ratio to see distribution simmilarity: $\\frac{p(X|D1)}{p(X|D2)}$\n",
    "* closer to one means distributions are closer\n",
    "\n",
    "Probability p(X|D1) = $p_{D1}(a)^{n_a} * p_{D1}(b)^{n_b} * p_{D1}(c)^{n_c}$ <br>\n",
    "Probability p(X|D2) = $p_{D2}(a)^{n_a} * p_{D2}(b)^{n_b} * p_{D2}(c)^{n_c}$\n",
    "\n",
    "Generalization:\n",
    "\n",
    "$$\\frac{p_{D1}(a)^{n_a} * p_{D1}(b)^{n_b} * p_{D1}(c)^{n_c}}{p_{D2}(a)^{n_a} * p_{D2}(b)^{n_b} * p_{D2}(c)^{n_c}}$$\n",
    "\n",
    "Now put a log on it and exponent to normalize for scaling factors and the sequence size:\n",
    "\n",
    "$$log(\\frac{p_{D1}(a)^{n_a} * p_{D1}(b)^{n_b} * p_{D1}(c)^{n_c}}{p_{D2}(a)^{n_a} * p_{D2}(b)^{n_b} * p_{D2}(c)^{n_c}})^{\\frac{1}{n}}$$\n",
    "\n",
    "Now simplify:\n",
    "\n",
    "$$\\frac{n_a}{n}log(p_{D1}(a)) + \\frac{n_b}{n}log(p_{D1}(b)) + \\frac{n_c}{n}log(p_{D1}(c)) - \\frac{n_a}{n}log(p_{D2}(a)) + \\frac{n_b}{n}log(p_{D2}(b)) + \\frac{n_c}{n}log(p_{D2}(c))$$\n",
    "\n",
    "Recall that we are taking our random draw X from  D1, so as N -> big the ratio $\\frac{n_a}{n} -> p_{D1}(a)$<br>\n",
    "**Note:** If we were to assume the random draw X was from D1 then the ration would collapse to $p_{D2}(a)$, instead. This is why KL divergences is **Asymetric**. $D_{KL}(D1 || D2) \\neq D_{KL}(D2 || D1)$\n",
    "\n",
    "$$p_{D1}(a)log(p_{D1}(a)) + p_{D1}(b)log(p_{D1}(b)) + p_{D1}(c)log(p_{D1}(c)) - p_{D1}(a)log(p_{D2}(a)) - p_{D1}(b)log(p_{D2}(b)) - p_{D1}(c)log(p_{D2}(c))$$ <br>\n",
    "\n",
    "re-arrange:\n",
    "\n",
    "$$p_{D1}(a)log(p_{D1}(a)) - p_{D1}(a)log(p_{D2}(a)) + p_{D1}(b)log(p_{D1}(b)) - p_{D1}(b)log(p_{D2}(b)) + p_{D1}(c)log(p_{D1}(c)) - p_{D1}(c)log(p_{D2}(c))$$ <br>\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$p_{D1}(a)log(\\frac{p_{D1}(a)}{p_{D2}(a)}) +p_{D1}(b)log(\\frac{p_{D1}(b)}{p_{D2}(b)}) + p_{D1}(c)log(\\frac{p_{D1}(c)}{p_{D2}(c)}) $$\n",
    "\n",
    "Generalize for more M classes instead of just 3:\n",
    "\n",
    "$$ \\sum_i^m p_{D1}(i)log(\\frac{p_{D1}(i)}{p_{D2}(i)}) $$\n",
    "\n",
    "Generalize for Continuous distributions instead of discreet:\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} p_{D1}(x)log(\\frac{p_{D1}(x)}{p_{D2}(x)}) dx$$\n",
    "\n",
    "or\n",
    "$$\\int_{x \\sim D1} p_{D1}(x)log(\\frac{p_{D1}(x)}{p_{D2}(x)}) dx$$\n",
    "\n",
    "Equivalent to \n",
    "\n",
    "$$D_{KL} = \\mathbb{E}_{x \\sim D1(x)} [log(\\frac{p_{D1}(x)}{p_{D2}(x)})]$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Take 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Motivation:**\n",
    "<br>\n",
    "Our purpose is to learn the generative\n",
    "process, i.e., p(x|z) (we assume p(z) is known). A good p(x|z) would assign high probabilities to\n",
    "observed x; hence, we can learn a good p(x|z) by maximizing the probability of observed data,\n",
    "i.e., p(x). Assuming that p(x|z) is parameterized by Î¸, we need to solve the following optimization\n",
    "problem. Which is to maximize the following:\n",
    "<br>\n",
    "\n",
    "$$p_\\theta(x) = \\int_zp_\\theta(x|z)p(z)dz $$\n",
    "\n",
    "\n",
    "\n",
    "Because that is the probability of $x \\cup z$. In this case, the x is our samples which we have \"drawn\" from the \"imaginary\" distribution z. In other words this is the probability of us getting the data that we have. Since our samples obviously exist, we want $p_\\theta(x)$ to be closer to 1.<br>\n",
    "Unfortunately, taking the integral over $p(x)$ is intractable (aka very computationally expensive) when dealing with high dimmensions.<br>\n",
    "<br>\n",
    "The solution is to actually take a step back and look at a different part of the VAE, and that is the decoder **$p_\\theta(z|x)$**, we can't actually integrate over that though since it is a latent mapping that we don't know. Instead the best we can do is try to apporoximate it with our own version **$q_\\phi(z|x)$**.<br>\n",
    "To do this we use **Variational Inference** with **KL divergence**.Aka, want to minimize the $D_{KL}(q_\\phi(z|x) || p_\\theta(z|x))$\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(q_\\theta(z|x) || p_\\theta(z|x)) & = \\int_{z}(q_\\phi(z|x)log(\\frac{q_\\phi(z|x)}{p_\\theta(z|x)}dz \\\\\n",
    "& = \\int_{z}(q_\\phi(z|x)log(\\frac{q_\\phi(z|x)p_\\theta(x)}{p_\\theta(z,x)})dz \\\\\n",
    "& = \\int_{z}(q_\\phi(z|x)\\bigg(log\\big(\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}\\big) + log\\big(p_\\theta(x)\\big)\\bigg)dz \\\\\n",
    "& = \\int_{z}(q_\\phi(z|x)\\bigg(log\\big(\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}\\big)\\bigg)dz + \\int_{z}(q_\\phi(z|x)log\\big(p_\\theta(x)\\big)dz \\\\\n",
    "D_{KL}(q_\\theta(z|x) || p_\\theta(z|x)) & = \\int_{z}(q_\\phi(z|x)\\bigg(log\\big(\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}\\big)\\bigg)dz + log\\big(p_\\theta(x)\\big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now use substitution with:\n",
    "$$-\\mathcal{L}(\\phi, \\theta) = \\int_{z}(q_\\phi(z|x)\\bigg(log\\big(\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}\\big)\\bigg)dz$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(q_\\theta(z|x) || p_\\theta(z|x)) & =  - \\mathcal{L}(\\phi, \\theta) + log\\big(p_\\theta(x)\\big) \\\\\n",
    "D_{KL}(q_\\theta(z|x) || p_\\theta(z|x)) + \\mathcal{L}(\\phi, \\theta) & =  log\\big(p_\\theta(x)\\big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since KL Divergence has to be >= 0, we can re-write this as:\n",
    "$$\\mathcal{L}(\\phi, \\theta) <=  log\\big(p_\\theta(x)\\big)$$\n",
    "\n",
    "Reminder that our original objective was to maximize $p_\\theta(x)$. We can do this by maximizing $\\mathcal{L}(\\phi, \\theta)$, which is the lower bound for $p_\\theta(x)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Useful formulas:\n",
    "\n",
    "$$p(a|b) = \\frac{p(b|a)p(a)}{p(b)}$$\n",
    "$$p(a|b)p(b) = p(a,b) = p(b,a) = p(b|a)p(a)$$\n",
    "\n",
    "$$\\mathbb{E}_{x \\sim D1} f() = \\int_xp(x)f()$$\n",
    "\n",
    "$$\\int_{z}(q_\\phi(z|x)f()dz = f()$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The maximization problem, to maximize $\\mathcal{L}(\\phi, \\theta)$ with respect to $\\phi$ and $\\theta$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$-\\mathcal{L}(\\phi, \\theta) = \\int_{z}(q_\\phi(z|x)\\bigg(log\\big(\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}\\big)\\bigg)dz$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}