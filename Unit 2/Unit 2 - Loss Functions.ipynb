{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb531f",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "| Loss Function | Formula | Notes | Uses |\n",
    "| --- | --- | --- | --- |\n",
    "| Hinge Loss | $$loss=maximum(1 - y_{true} * y_{pred}, 0)$$ | Originally developed for SVM, hinge loss encourages examples to have the correct sigh by assigning more error to labels that have a difference in sign between the predicted and actual. This results in implicit behavior that maximizes the margin in neural networks much like with SVMs | SVM, Binary Classification |\n",
    "| Huber Loss | $$\\text{For each value x in error = y_true - y_pred} \\begin{cases} 0.5 * x^2 \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{if |x| <= d} \\\\ 0.5 * d^2 + d * (|x| - d) \\quad \\text{if |x| > d}\\end{cases}$$ | Offers a strong balance between MAE and MSE and combines the best of both loss functions as long as delta (d) is chose properly. Delta is a parameter chosen by the user that controls the behavior of the loss function. As delta approaches infinity, huber approaches MSE and when it approaches 0 it approaches MAE | Regression |\n",
    "| Mean Absolute Error (MAE) | $$loss=abs(y_{true}-y_{pred})$$ | MAE places all of our errors on the same weighted linear scale and because of this, this allows MAE to be insensitive when it comes towards handling outliers. MAE is computationally expensive and may have local minima as downsides. | Regression | \n",
    "| Mean Squared Error (MSE) | $$loss=(y_{true}-y_{pred})^2$$ | Acts as an almost inverse to MAE. MSE is strong at predicting outliers since it places a larger weight on these errors but this is also a double edged sword as the squaring of errors can also allow the function to magnify errors to be very large leading to strange performance. | Regression|\n",
    "| Binary Cross-Entropy | $$ loss = -\\frac{1}{m}\\sum_{i=1}^{m}{y_{i}*log(\\hat{y}_{i})} $$ | Probablistic loss function that is commonly used for two class problems. m is used to represent the amount of records and y hat represents the predicted probability distribuytions for predicting one class | Binary Classification | \n",
    "| Categorical Cross-Entropy | $$ loss = -\\frac{1}{m}\\sum_{i=1}^{N}\\sum_{c=1}^{C} 1_{y_{i}\\in C_{c}}logp_{model}[y_i \\in C_c] $$ | Probablistic loss function that is commonly used for multi-class problems. Expects labels to be provided in one_hot encoding. Ex [1,0,0], [0,0,1], [0,0,1]| Multi-class Classification | \n",
    "| Sparse Categorical Cross-Entropy | $$ loss = -\\frac{1}{m}\\sum_{i=1}^{N}\\sum_{c=1}^{C} 1_{y_{i}\\in C_{c}}logp_{model}[y_i \\in C_c] $$ | Probablistic loss function that is commonly used for multi-class problems. Has same loss function as Categorical Cross-Entropy only difference is that it expects labels to be provided in integer encoding. Ex. [1], [2], [3] | Multi-class Classification | \n",
    "| Cosine Similarity | $$\\text{loss = -sum(l2_norm(y_true) * l2_norm(y_pred))}$$ | Cosine Similarity is a measurement that quantifies the similarity between two or more vectors. Note that it is a number between -1 and 1. When it is a negative number between -1 and 0, 0 indicates orthogonality and values closer to -1 indicate greater similarity. The values closer to 1 indicate greater dissimilarity. This makes it usable as a loss function in a setting where you try to maximize the proximity between predictions and targets. If either y_true or y_pred is a zero vector, cosine similarity will be 0 regardless of the proximity between predictions and targets. | Similarity tasks (e.g. compare two articles) | \n",
    "| KLDivergence | $$\\text{loss = y_true * log(y_true / y_pred)}$$ | Used to measure the difference between two probability distributions over the same variable x. | VAE, Multi-class, Regression |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a7f6b",
   "metadata": {},
   "source": [
    "See for more on cross entropy:\n",
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "\n",
    "Explaination on Categorical Cross-Entropy per https://github.com/keras-team/keras/issues/6444:\n",
    "\"The double sum is over the observations `i`, whose number is `N`, and the categories `c`, whose number is `C`. The term `1_{y_i \\in C_c}` is the indicator function of the `i`th observation belonging to the `c`th category. The `p_{model}[y_i \\in C_c]` is the probability predicted by the model for the `i`th observation to belong to the `c`th category. When there are more than two categories, the neural network outputs a vector of `C` probabilities, each giving the probability that the network input should be classified as belonging to the respective category. When the number of categories is just two, the neural network outputs a single probability `\\hat{y}_i`, with the other one being `1` minus the output. This is why the binary cross entropy looks a bit different from categorical cross entropy, despite being a special case of it.\"\n",
    "\n",
    "See for more on klDivergence:\n",
    "- https://dibyaghosh.com/blog/probability/kldivergence.html\n",
    "- https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-kullback-leibler-divergence-kl-divergence-with-keras.md\n",
    "\n",
    "Sources:\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20c983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
