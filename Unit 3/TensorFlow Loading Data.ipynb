{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pressing-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras as tk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-national",
   "metadata": {},
   "source": [
    "## Loading Data as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-preservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_mnist = tk.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prepared-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow_datasets # May need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-retreat",
   "metadata": {},
   "source": [
    "## Using the Dataset API\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset<br>\n",
    "The tensorflow datasets api has lots of features, so it is definetly worth it to have a cursory look through the methods outlined in the above link. However, I have included what I find to be the most useful below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "matched-event",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 16:39:07.565700: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-19 16:39:10.443964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9640 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2022-02-19 16:39:10.445617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9640 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:1b:00.0, compute capability: 7.5\n",
      "2022-02-19 16:39:10.447130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9640 MB memory:  -> device: 2, name: GeForce RTX 2080 Ti, pci bus id: 0000:60:00.0, compute capability: 7.5\n",
      "2022-02-19 16:39:10.448601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9640 MB memory:  -> device: 3, name: GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "overall-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "class_names = metadata.features['label'].names\n",
    "print(\"Class names: {}\".format(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parental-penalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples:     10000\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = metadata.splits['train'].num_examples\n",
    "num_test_examples = metadata.splits['test'].num_examples\n",
    "print(\"Number of training examples: {}\".format(num_train_examples))\n",
    "print(\"Number of test examples:     {}\".format(num_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0550d8-6381-43e6-a9d9-83bc893d22b7",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Caching makes it so that data is read and stored after the first iteration (epoch 1), either in memory or some file. In the next pass throught the data it will be read from this location, which should be faster.\n",
    "**Note: Caching stores data as it it used, so if you are chaining some kind of transformation or function on the data like \".map\" then put those before the .cache(). However, put somthing like .shuffle() after the cache, otherwise it will only shuffle in the first iteration. Example below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7bee3-05e0-4a88-a364-986958d4c3a5",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Just specifies how many samples the dataset will package and output everytime it is asked for a new batch. Specifying it here means it doesn't need to be specified during the model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ae73d-74d1-4636-81ec-91d7246bb4a9",
   "metadata": {},
   "source": [
    "### Map\n",
    "Simply applys some function/transformation to each element. Very useful for things like normalization or calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17292a35-454d-42ea-aea1-0cf5b108d0d5",
   "metadata": {},
   "source": [
    "### Prefetch\n",
    "Prefetch is a super useful feature, it allows the next batch of data to be pulled in and prepared while the current batch is being processed by the model. This improves throughput of the training process. Most dataset input pipelines should end with a call to prefetch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c839ab-3659-4109-91ac-a409f465440d",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "This loads a specified number of samples into a buffer and then shuffles them before selecting a batch. Selected samples get replaced with new samples from the rest of the dataset. In order for the entire dataset to be shuffled traditionally the buffer_size needs to be equal or greater than the total number of sampels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cda90ed-113d-4a73-b0c8-5ad915aca817",
   "metadata": {},
   "source": [
    "**Note** You can often pass \"tf.data.AUTOTUNE\" as a parameter for many of this dataset functions and Tensorflow will automaitcally try to optimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comic-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images, labels):\n",
    "  images = tf.cast(images, tf.float32)\n",
    "  images /= 255\n",
    "  return images, labels\n",
    "\n",
    "train_dataset =  train_dataset.map(normalize).cache().shuffle(200).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset  =  test_dataset.map(normalize).cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wrong-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11572832-5b9b-41b3-b50e-17d8eba564f6",
   "metadata": {},
   "source": [
    "## Loading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6583bf9-45d3-493c-8876-07f21abc5613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e280ef-ba9e-4ed8-a5eb-eb040a0c29bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features = titanic.copy()\n",
    "titanic_labels = titanic_features.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca36857-2db0-40cc-b3c6-74fc5fd0a8c1",
   "metadata": {},
   "source": [
    "## **tf.data.Datesets.from_tensor_slices()**\n",
    "This will probably be used if making your own datasets from \"wild\" data. So pay attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044515ab-b73a-49b0-8584-299e4f7acc6e",
   "metadata": {},
   "source": [
    "As you are probably familiar with, you can just go ahead and use the titanic_features and titanic_labels as your x and y right now. However, you could also take this a step forward and turn this csv data into an actual tf.data.Dataset to leverage all the useful features that API gives you.\n",
    "\n",
    "In order to do this the first step is to convert the dataframe into a **dictionary of tensors**. This is a dictionary where the feature names (columns) are the keys and the associated values are the entire array of values for that feature. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7df299a8-ab98-4463-a26d-fbb5621018c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dataframe into dictionary of tensors\n",
    "titanic_features_dict = {name: np.array(value) \n",
    "                         for name, value in titanic_features.items()}\n",
    "# titanic_features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37185deb-47e6-4592-b7b6-baa51774bcc7",
   "metadata": {},
   "source": [
    "This needs to be done in order to use the tf.data.Datesets.from_tensor_slices() function as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4bb7d68-f1d2-4673-a2f1-086d606a12cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                : b'male'\n",
      "age                : 22.0\n",
      "n_siblings_spouses : 1\n",
      "parch              : 0\n",
      "fare               : 7.25\n",
      "class              : b'Third'\n",
      "deck               : b'unknown'\n",
      "embark_town        : b'Southampton'\n",
      "alone              : b'n'\n"
     ]
    }
   ],
   "source": [
    "features_dataset_no_labels = tf.data.Dataset.from_tensor_slices(titanic_features_dict)\n",
    "\n",
    "for example in features_dataset_no_labels:\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215e3b5-abee-46ec-a4c1-e6c738848127",
   "metadata": {},
   "source": [
    "tf.data.Datesets.from_tensor_slices() is flexible, here is a way to do it with the labels as part of the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa460b80-ef62-492a-8f55-0202c38f1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t They are Dead :(\n",
      "sex                : b'male'\n",
      "age                : 22.0\n",
      "n_siblings_spouses : 1\n",
      "parch              : 0\n",
      "fare               : 7.25\n",
      "class              : b'Third'\n",
      "deck               : b'unknown'\n",
      "embark_town        : b'Southampton'\n",
      "alone              : b'n'\n"
     ]
    }
   ],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))\n",
    "\n",
    "for example, label in features_dataset:\n",
    "  print(f'\\t They are {\"Alive! :)\" if label else \"Dead :(\"}')\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459933d-2ada-43bc-a82c-84a94b05211e",
   "metadata": {},
   "source": [
    "There is also experimental support for creating a dataset directly from a dataset without reading it into memory and turning it into a dataframe first. Read about it here: https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbbfac-1574-443c-8bf0-5cd224d32994",
   "metadata": {},
   "source": [
    "#### For very large datasets tensorflow also alows for creating dataset objects that are connected directly to files and can read from the disk.\n",
    "This is an advance topic but details can be found here: https://www.tensorflow.org/guide/data#basic_mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dda372-6cbc-4658-a75c-f14891797b04",
   "metadata": {},
   "source": [
    "# tf.keras.layers preprocessing layers\n",
    "Complete guide here: https://www.tensorflow.org/guide/keras/preprocessing_layers#quick_recipes <br>\n",
    "Useful examples: https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n",
    "\n",
    "These preprocessing layers allow for things like normalization and one-hot encoding to be done as part of the model itself. This makes the model a truly end-to-end solution and makes it much more portable from platform and environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-decade",
   "metadata": {},
   "source": [
    "### tf.normalization method\n",
    "\n",
    "Allows the normalization process to be encorporated into being part of the model, allowing the dataset to be untouched. Normalizing is important because even Neural nets will converge faster if the mean of features is near 0.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe707076-c917-4d31-ab53-673f9a0b3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[-4.         -3.04761905 -2.0952381  -1.14285714 -0.19047619  0.76190476\n",
      "   1.71428571  2.66666667  3.61904762  4.57142857  5.52380952]\n",
      " [-3.52380952 -2.57142857 -1.61904762 -0.66666667  0.28571429  1.23809524\n",
      "   2.19047619  3.14285714  4.0952381   5.04761905  6.        ]]\n",
      "r: [[-3.         -2.04761905 -1.0952381  -0.14285714  0.80952381  1.76190476\n",
      "   2.71428571  3.66666667  4.61904762  5.57142857  6.52380952]\n",
      " [-2.52380952 -1.57142857 -0.61904762  0.33333333  1.28571429  2.23809524\n",
      "   3.19047619  4.14285714  5.0952381   6.04761905  7.        ]]\n",
      "tf.Tensor(\n",
      "[[-1.5811388  -1.5811388 ]\n",
      " [-1.264911   -1.264911  ]\n",
      " [-0.9486833  -0.9486833 ]\n",
      " [-0.6324556  -0.6324556 ]\n",
      " [-0.31622776 -0.3162278 ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.31622776  0.31622773]\n",
      " [ 0.6324556   0.63245547]\n",
      " [ 0.9486833   0.9486833 ]\n",
      " [ 1.2649112   1.2649109 ]\n",
      " [ 1.5811388   1.5811388 ]], shape=(11, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.2490996  -1.2490996 ]\n",
      " [-0.93287194 -0.93287194]\n",
      " [-0.61664414 -0.61664414]\n",
      " [-0.30041638 -0.30041638]\n",
      " [ 0.01581139  0.01581137]\n",
      " [ 0.33203915  0.33203915]\n",
      " [ 0.6482669   0.6482669 ]\n",
      " [ 0.9644947   0.96449465]\n",
      " [ 1.2807225   1.2807225 ]\n",
      " [ 1.5969503   1.5969502 ]\n",
      " [ 1.913178    1.913178  ]], shape=(11, 2), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 16:39:10.934311: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.7619048, 1.2380953]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[9.070294, 9.070294]], dtype=float32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x represents a 11 samples each with 2 features\n",
    "# r is just there to show the layer can be applied to other input\n",
    "\n",
    "x = np.linspace(-4, 6, 22).reshape(11, 2)\n",
    "r = x +1\n",
    "print(f'x: {x.T}')\n",
    "print(f'r: {r.T}')\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(x)\n",
    "print(norm_layer(x))\n",
    "print(norm_layer(r))\n",
    "norm_layer.mean, norm_layer.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4035c-7eac-49c5-8aa3-218533f34c5a",
   "metadata": {},
   "source": [
    "If all of the data is numerical then a normalization layer can just be adapted to the entire training data and simply added as the first layer of the tensorflow model. **Easy**.<br>\n",
    "However, if some of the features are catagorical then things get a little more complicated. The good news is that there are also preprocessing layers that allow for things like the model automatically converting strings into one hot encodings. The bad news is that doing this will require using the keras **functional api** which is more complicated than the sequential one. If you have not reviewed the **functional vs sequential api** notes yet, it is recommended to understand that first before trying this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-consumer",
   "metadata": {},
   "source": [
    "### Keras.Input objects for Symbolic representation\n",
    "keras.input objects are simply used to create symbolic placeholder Keras tensors. <br>\n",
    "First, create keras.Input objects to represent each of your features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26104fdb-3ea2-4491-ae35-e8a99d8e23ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sex': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'sex')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'age')>,\n",
       " 'n_siblings_spouses': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'n_siblings_spouses')>,\n",
       " 'parch': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'parch')>,\n",
       " 'fare': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'fare')>,\n",
       " 'class': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'class')>,\n",
       " 'deck': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'deck')>,\n",
       " 'embark_town': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'embark_town')>,\n",
       " 'alone': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'alone')>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {}\n",
    "\n",
    "for name, column in titanic_features.items():\n",
    "  dtype = column.dtype\n",
    "  if dtype == object:\n",
    "    dtype = tf.string\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00232e0a-5b73-43cb-a0e2-ab66f97fc6dd",
   "metadata": {},
   "source": [
    "Then isolate all the numeric features and create a normalization layer just for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "208d31e2-1a52-4154-86e4-dd91d4949f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'normalization_1')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.float32}\n",
    "\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
    "all_numeric_inputs = norm(x)\n",
    "\n",
    "all_numeric_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440e1df-396b-4d76-8e9b-e7423f342450",
   "metadata": {},
   "source": [
    "## tf.keras.layers.StringLookup\n",
    "## tf.keras.layers.CategoryEncoding\n",
    "The StringLookup layer automatically fits a dictionary on string data and then uses that to encode incoming data with numeric labels. If you don't know how dictionaries work in nlp then refer to those notes.<br>\n",
    "The Category Encoding layer then converts numeric input into one-hot encoding, although it can also be used for multi-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50006f7f-99ee-45ea-a285-4df18a936301",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "all_inputs.append(all_numeric_inputs)\n",
    "for name, input in inputs.items():\n",
    "  if input.dtype == tf.float32:\n",
    "    continue\n",
    "\n",
    "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
    "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "\n",
    "  x = lookup(input)\n",
    "  x = one_hot(x)\n",
    "  all_inputs.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c95f2a5-2967-45d4-a0e6-ec63aad302fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "age (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "n_siblings_spouses (InputLayer) [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parch (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fare (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sex (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "class (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deck (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embark_town (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "alone (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4)            0           age[0][0]                        \n",
      "                                                                 n_siblings_spouses[0][0]         \n",
      "                                                                 parch[0][0]                      \n",
      "                                                                 fare[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup (StringLookup)    (None, 1)            0           sex[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_1 (StringLookup)  (None, 1)            0           class[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_2 (StringLookup)  (None, 1)            0           deck[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_3 (StringLookup)  (None, 1)            0           embark_town[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "string_lookup_4 (StringLookup)  (None, 1)            0           alone[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 4)            9           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding (CategoryEnco (None, 3)            0           string_lookup[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_1 (CategoryEn (None, 4)            0           string_lookup_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_2 (CategoryEn (None, 9)            0           string_lookup_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_3 (CategoryEn (None, 5)            0           string_lookup_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "category_encoding_4 (CategoryEn (None, 3)            0           string_lookup_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28)           0           normalization_1[0][0]            \n",
      "                                                                 category_encoding[0][0]          \n",
      "                                                                 category_encoding_1[0][0]        \n",
      "                                                                 category_encoding_2[0][0]        \n",
      "                                                                 category_encoding_3[0][0]        \n",
      "                                                                 category_encoding_4[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 9\n",
      "Trainable params: 0\n",
      "Non-trainable params: 9\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#the layers,Concatenate just takes in a list of tensors and returns one combined tensor\n",
    "preprocessed_inputs_cat = layers.Concatenate()(all_inputs)\n",
    "# defining a model with the functional API\n",
    "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "\n",
    "titanic_preprocessing.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77038cf6-3295-437b-8e56-4c355bfb7df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sex': array(['male'], dtype=object), 'age': array([22.]), 'n_siblings_spouses': array([1]), 'parch': array([0]), 'fare': array([7.25]), 'class': array(['Third'], dtype=object), 'deck': array(['unknown'], dtype=object), 'embark_town': array(['Southampton'], dtype=object), 'alone': array(['n'], dtype=object)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 28), dtype=float32, numpy=\n",
       "array([[-0.610415 ,  0.395198 , -0.4790527, -0.4974028,  0.       ,\n",
       "         0.       ,  1.       ,  0.       ,  0.       ,  0.       ,\n",
       "         1.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  1.       ,\n",
       "         0.       ,  0.       ,  0.       ,  1.       ,  0.       ,\n",
       "         0.       ,  1.       ,  0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
    "print(features_dict)\n",
    "titanic_preprocessing(features_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c605d2-3d91-4029-9a47-46e61ae39607",
   "metadata": {},
   "source": [
    "Below we actually put everything together and create this new model using both the Sequential and Functional API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pursuant-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def titanic_model(preprocessing_head, inputs):\n",
    "  body = tf.keras.Sequential([\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  preprocessed_inputs = preprocessing_head(inputs)\n",
    "  result = body(preprocessed_inputs)\n",
    "  model = tf.keras.Model(inputs, result)\n",
    "\n",
    "  model.compile(loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "                optimizer=tf.optimizers.Adam())\n",
    "  return model\n",
    "\n",
    "titanic_model = titanic_model(titanic_preprocessing, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713a1ddc-1911-4f54-8aba-f6f7fc24c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 1s 2ms/step - loss: 0.6331\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5371\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4911\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4622\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4460\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4349\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4289\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4250\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4246\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b881e92b100>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
